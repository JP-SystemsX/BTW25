{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:02:55.549774Z",
     "start_time": "2024-10-13T15:02:55.475212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the Dataset, it only works on Linux Systems like Colab \n",
    "# TODO: For BTW25 your own Dataset will be required\n",
    "!wget https://ember-climate.org/app/uploads/2023/12/european_wholesale_electricity_price_data_hourly.zip # For Win: Download dataset with link, unzip, and put into flderdefined below\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"european_wholesale_electricity_price_data_hourly.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../data\")"
   ],
   "id": "1ce190879b231b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:35:11.395418Z",
     "start_time": "2024-10-13T15:35:02.954215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ],
   "id": "7a2a53485f779680",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bd735d46d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:35:11.410385Z",
     "start_time": "2024-10-13T15:35:11.398387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define CLI Arguments\n",
    "SEQ_LENGTH = 256  # We will use 24 hours of data to predict the next hour\n",
    "batch_size = 128\n",
    "hidden_layer_size=128\n",
    "num_layers=6\n",
    "epochs = 5  \n",
    "target_column = \"Price (EUR/MWhe)\""
   ],
   "id": "bb48f22e2802c98c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:35:11.425799Z",
     "start_time": "2024-10-13T15:35:11.412384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the series into sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Our LSTM Architecture\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=num_layers, batch_first=True, dropout=0.1)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_seq, hidden_cell=None):\n",
    "        \"\"\"\n",
    "        Predicts what the next value of the given Sequence will be\n",
    "        Futher Work: Predict Several Time Steps ahead e.g. autoregressively by using an internal loop or by predicting a vector\n",
    "        :param input_seq: \n",
    "        :param hidden_cell: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        if hidden_cell is None:\n",
    "            # Initialize hidden and cell state\n",
    "            h0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(input_seq.device)\n",
    "            c0 = torch.zeros(self.num_layers, input_seq.size(0), self.hidden_layer_size).to(input_seq.device)\n",
    "            hidden_cell = (h0, c0)\n",
    "        lstm_out, (hn, cn) = self.lstm(input_seq, hidden_cell)\n",
    "        # First input to the decoder is the first time step of the target\n",
    "        lstm_out = lstm_out[:, -1, :].unsqueeze(1)  # (batch_size, 1, output_dim)\n",
    "        predictions = self.linear(lstm_out.reshape(lstm_out.size(0), lstm_out.size(1)* lstm_out.size(2)))\n",
    "        return self.activation(predictions), (hn, cn)  # Return the last prediction and Hidden State"
   ],
   "id": "57fad24d5a9d9df2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:35:11.658896Z",
     "start_time": "2024-10-13T15:35:11.427799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset \n",
    "data_path = \"../data/european_wholesale_electricity_price_data_hourly/Germany.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocessing\n",
    "prices = df[target_column].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "prices_scaled = scaler.fit_transform(prices)\n",
    "\n",
    "X, y = create_sequences(prices_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float()\n",
    "# Create Dataset from Tensors\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "8dbfa02bcb531b7d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T15:35:43.916567Z",
     "start_time": "2024-10-13T15:35:11.662970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size=1, hidden_layer_size=hidden_layer_size, output_size=1, num_layers=num_layers)\n",
    "# Check For GPU -> If available send model to it\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device) \n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())  # Classical Adam Optimizer with Weight Decay (i.e. early on high learning rate, later lower)\n",
    "print(f\"GPU Available: {torch.cuda.is_available()} (Use {device})\")"
   ],
   "id": "aa40aff166c04fad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True (Use cuda)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-13T15:35:43.916567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    # Train Step\n",
    "    model.train()\n",
    "    for seq, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred, _ = model(seq.to(device))\n",
    "        \n",
    "        loss = loss_function(y_pred.to(\"cpu\"), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "    # Validation Step (When this doesn't drop anymore then it starts overfitting)\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    errors = []\n",
    "    for seq, labels in tqdm(val_loader):\n",
    "    \n",
    "        y_pred, _ = model(seq.to(device))\n",
    "        loss = loss_function(input=y_pred.to(\"cpu\"), target=labels)\n",
    "        error = root_mean_squared_error(y_pred=y_pred.detach().to(\"cpu\"), y_true=labels)\n",
    "        val_losses.append(loss)\n",
    "        errors.append(error)\n",
    "    print(f'Epoch {epoch+1}:\\nTrain_Loss: {sum(train_losses)/len(train_losses)},\\nVal_Loss: {sum(val_losses)/len(val_losses)}\\nRMSE: {sum(errors)/len(errors)}')\n",
    "\n",
    "\n",
    "print(f'Training completed.')"
   ],
   "id": "2dd4720d33423b15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f5950ae62864a2f9c32cf07d4e3553c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca55a13d6f44482aa1f6b816465c2ae6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train_Loss: 0.0013092599110677838,\n",
      "Val_Loss: 0.010840397328138351\n",
      "RMSE: 0.07337536098156858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fed04a15178744f7ac7ddb9dc9a112e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fcf182151e64a7eb292d4a1caca1eb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The loss and RMSE are quite small\n",
    "- For that, it is an absolutely trivial LSTM; it already performs quite well\n",
    "- Reason: By Time-Series, the reason for this is often that the next value is likely very similar to the last few values\n",
    "- Meaning: If you make only minor (or no adjustments) at all to the last value, you already have a pretty good forecast\n",
    "- However: The Problem with this approach becomes salient if we forecast several time steps ahead"
   ],
   "id": "c79727fa9fa5e037"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Warning: The Plotting is often the most annoying part\n",
    "\n",
    "# Function to create auto regressive predictions\n",
    "def autoregressive_forecast(model, data, forecast_length, hidden_state=None):\n",
    "    model.eval()  # Set the model to evaluation mode (else it calculates Gradients)\n",
    "    \n",
    "    predictions = []\n",
    "    input_seq = data.to(device) \n",
    "    \n",
    "    for _ in range(forecast_length):\n",
    "        # Forward pass through the model to predict the next step\n",
    "        with torch.no_grad():\n",
    "            y_pred, _ = model(input_seq, None)\n",
    "        \n",
    "        # Store the prediction\n",
    "        predictions.append(y_pred.item())\n",
    "        \n",
    "        # Update the input sequence (drop the first element and append the new prediction)\n",
    "        input_seq = torch.cat((input_seq[:,1:,:], y_pred.unsqueeze(0)), dim=1)\n",
    "        # Note: Alternatively one could also feed back the hidden state and only the prediction - One should end up with about the same result but this approach here seems more intuitive\n",
    "    return predictions\n",
    "\n",
    "def plot_forecast(forecast_length = 8):\n",
    "    # Get last SEQ_LENGTH values but exclude forecast_length many values to validate forecast later on\n",
    "    input_seq = prices_scaled[-(SEQ_LENGTH+forecast_length):-forecast_length]\n",
    "    input_seq = torch.tensor(input_seq).float().unsqueeze(0)\n",
    "    # Get last forecast_length many values to validate prediction\n",
    "    target = prices_scaled[-forecast_length-1:]  \n",
    "    \n",
    "    # Make Prediction\n",
    "    autoregressive_predictions = autoregressive_forecast(model, input_seq, forecast_length=forecast_length, hidden_state=None)\n",
    "    \n",
    "    # Rescale the prediction and target values back to original scale\n",
    "    pred = scaler.inverse_transform(np.array(autoregressive_predictions).reshape(-1, 1))\n",
    "    target = scaler.inverse_transform(target)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(forecast_length), pred, label='Predicted (Autoregressive)', linestyle='--')\n",
    "    plt.plot(range(forecast_length), target[1:], label='Actual (Test Set)', linestyle='-')\n",
    "    plt.title('Autoregressive Forecast vs Actual Test Set Values')\n",
    "    plt.xlabel('Time Step (Hour)')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the results but include Input Sequence\n",
    "    input_seq_rescaled = scaler.inverse_transform(input_seq.reshape(-1, 1))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(SEQ_LENGTH), input_seq_rescaled, label='Input Sequence', linestyle='-')\n",
    "    plt.plot(range(SEQ_LENGTH, SEQ_LENGTH + forecast_length), pred, label='Predicted (Autoregressive)', linestyle='--')\n",
    "    plt.plot(range(SEQ_LENGTH-1, SEQ_LENGTH + forecast_length), target, label='Actual (Test Set)', linestyle='-')\n",
    "    plt.title('Input Data, Autoregressive Forecast, and Actual Test Set Values')\n",
    "    plt.xlabel('Time Step (Hour)')\n",
    "    plt.ylabel('Electricity Price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "c5d307b8c5ff4093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "plot_forecast(7)",
   "id": "82f1f6e39ab6c1ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Seems pretty accurate\n",
    "Let's look at a longer forecast"
   ],
   "id": "b74c1a4fe02dcae1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "plot_forecast(24)",
   "id": "a6d82594960f36cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This Forecast is much worse, likely due to:\n",
    "- Harder Startpoint\n",
    "- Errors add up over time\n",
    "# LSTM don't pick up on the Season\n",
    "- The first few forecasts of the LSTM are pretty good\n",
    "- Our LSTM seems not to have picked up on the season nor trend\n",
    "- Hence the forecast becomes pretty much worthless "
   ],
   "id": "fabdf395badc4f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Further Work Ideas\n",
    "- Ensemble Methods \n",
    "    - AutoGluon found that traditional ML Methods (SupportVector Machines, Decision Trees, ETS, AutoArima) are pretty good on picking up the rough patterns \n",
    "    - In Forecasting this would be trend and seasonality \n",
    "    - Neural Networks were found helpful to make fine adjustments to those forecast\n",
    "- External Influences\n",
    "    - Often External Influences help to make more appropriate Predictions\n",
    "    - e.g. Knowing how the weather will be tomorrow (or today at least) should help with predicting how much a Solar Plant will produce then\n",
    "- Sequence To Sequence \n",
    "    - There are two standard approaches to avoid regression to the mean (at least for a while)\n",
    "    - Training Several Models one for each time step e.g. Model A predicts the next hour, Model B the hour after that, and so on (basically every model predicts just one step ahead just at different frequencies) \n",
    "    - Training a Sequence to Sequence model - A model that predicts the next n forecasts at once e.g. using several outputs on the last linear layer\n",
    "        - Alternatively: Training a model for making longer forecasts in AR fashion, usually not that good but better than vanilla LSTM on AR tasks\n",
    "- __Upgrade to xLSTM__\n",
    "    - LSTM have become outdated by now due to the advent of Transformers that can do many tasks better than LSTMs (not all though)\n",
    "    - Hence a paper called \"eXtended Long Short Term Memory\" introduced adjustments to the standard LSTM architecture that make them competitive again\n",
    "    - Github Repo: https://github.com/nx-ai/xlstm\n",
    "- Hyperparameter Optimization & Neural Architecture Search:\n",
    "    - Hyperparameter Optimization\n",
    "        - Usually Optimizing the Hyperparameters like Learning Rate, Batch Size, dropout, activation functions, Layer Sizes, etc. leads already to significant improvements\n",
    "        - This process can be automatized with algorithms which test several configurations and choose the best\n",
    "    - Neural Architecture Search\n",
    "        - One can go also one step further and test several different Neural Architectures\n",
    "        - e.g. Adding Layers, Removing Layers, Changing their size, their order, etc.\n",
    "        - This takes longer than just Hyperparameter Optimization but can lead to novel high performing solutions\n",
    "    - For Both NNI is to recommend as a library as it offers:\n",
    "        - Solid Search algorithms that speed up the optimization process\n",
    "        - A relatively flat learning curve\n",
    "        - Integrated Neural Architecture Search support (especially Pytorch)\n",
    "        - A Nice dashboard to analyse findings easily"
   ],
   "id": "d39d59840ebcae1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f76254dcf6ccd377",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
